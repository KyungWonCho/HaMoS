<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <meta name="title" content="Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="We present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="HaMoS, Computer Vision, Egocentric Vision, Human Motion Reconstruction, Diffusion Model">
  <!-- TODO: List all authors -->
  <meta name="author" content="Kyungwon Cho, Hanbyul Joo">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="We present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://kyungwoncho.github.io/HaMoS">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://kyungwoncho.github.io/HaMoS/static/images/thumbnail.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="720">
  <meta property="og:image:alt" content="HaMoS Project Preview">
  <meta property="article:published_time" content="2025-12-23T12:00:00.000Z">
  <meta property="article:author" content="Kyungwon Cho">
  <meta property="article:section" content="Computer Vision and Pattern Recognition">
  <meta property="article:tag" content="Egocentric Vision">
  <meta property="article:tag" content="Motion Reconstruction">

  <!-- Twitter -->
  <meta name="twitter:card" content="summary_large_image">
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <meta name="twitter:site" content="@kyungwon__cho">
  <!-- TODO: Replace with first author's Twitter handle -->
  <meta name="twitter:creator" content="@kyungwon__cho">
  <!-- TODO: Same as paper title above -->
  <meta name="twitter:title" content="Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context">
  <!-- TODO: Same as description above -->
  <meta name="twitter:description" content="We present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices.">
  <!-- TODO: Same as social preview image above -->
  <meta name="twitter:image" content="https://kyungwoncho.github.io/HaMoS/static/images/thumbnail.png">
  <meta name="twitter:image:alt" content="HaMoS Project Preview">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context">
  <meta name="citation_author" content="Cho, Kyungwon">
  <meta name="citation_author" content="Joo, Hanbyul">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="arXiv">
  <meta name="citation_pdf_url" content="https://arxiv.org/pdf/2512.19283.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link rel="apple-touch-icon" href="static/images/icon.png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context",
    "description": "We present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices.",
    "author": [
      {
        "@type": "Person",
        "name": "Kyungwon Cho",
        "affiliation": {
          "@type": "Organization",
          "name": "Seoul National University"
        }
      },
      {
        "@type": "Person",
        "name": "Hanbyul Joo",
        "affiliation": {
          "@type": "Organization",
          "name": "Seoul National University"
        }
      }
    ],
    "datePublished": "2024-12-23",
    "publisher": {
      "@type": "Organization",
      "name": "arXiv"
    },
    "url": "https://kyungwoncho.github.io/HaMoS",
    "image": "https://kyungwoncho.github.io/HaMoS/static/images/thumbnail.png",
    "keywords": ["Egocentric Vision", "Motion Reconstruction", "Computer Vision", "Diffusion Model", "HaMoS"],
    "abstract": "Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding.",
    "citation": "@article{cho2025hamos, title={Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context}, author={Cho, Kyungwon and Joo, Hanbyul}, journal={arXiv preprint arXiv:2512.19283}, year={2025}}",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://kyungwoncho.github.io/HaMoS"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Computer Vision"
      },
    ]
  }
  </script>
  
  <!-- Website/Organization Structured Data -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Seoul National University",
    "url": "https://kyungwoncho.github.io/",
    "logo": "https://kyungwoncho.github.io/HaMoS/static/images/icon.png",
    "sameAs": [
      "https://twitter.com/kyungwon__cho",
      "https://github.com/KyungWonCho"
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title" style="font-size: 2.6rem;">Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a href="https://kyungwoncho.github.io/" target="_blank">Kyungwon Cho</a>,</span>
              <span class="author-block">
                <a href="https://jhugestar.github.io/" target="_blank">Hanbyul Joo</a></span>  
            </div>

            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your institution and conference/journal info -->
              <span class="author-block">Seoul National University<br></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- TODO: Replace with your GitHub repository URL -->
                <span class="link-block">
                  <a href="https://github.com/KyungWonCho/HaMoS" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2512.19283" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body" style="padding-top: 0;">
      <!-- TODO: Replace with your teaser video -->
      <video poster="" id="tree" autoplay muted loop playsinline height="100%" preload="metadata">
        <!-- TODO: Add your video file path here -->
        <source src="static/videos/teaser_final.mp4" type="video/mp4">
      </video>
      <!-- TODO: Replace with your video description -->
      <h2 class="subtitle has-text-centered mt-2">
        We present the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and
intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            Egocentric vision systems are becoming widely available, creating new opportunities for human-computer interaction. A core challenge is estimating the wearer's full-body motion from first-person videos, which is crucial for understanding human behavior. However, this task is difficult since most body parts are invisible from the egocentric view. Prior approaches mainly rely on head trajectories, leading to ambiguity, or assume continuously tracked hands, which is unrealistic for lightweight egocentric devices. In this work, we present HaMoS, the first hand-aware, sequence-level diffusion framework that directly conditions on both head trajectory and intermittently visible hand cues caused by field-of-view limitations and occlusions, as in real-world egocentric devices. To overcome the lack of datasets pairing diverse camera views with human motion, we introduce a novel augmentation method that models such real-world conditions. We also demonstrate that sequence-level contexts such as body shape and field-of-view are crucial for accurate motion reconstruction, and thus employ local attention to infer long sequences efficiently. Experiments on public benchmarks show that our method achieves state-of-the-art accuracy and temporal smoothness, demonstrating a practical step toward reliable in-the-wild egocentric 3D motion understanding.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Video carousel -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-2">Long Demo</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/first_web.mp4" type="video/mp4">
          </video>
          <div class="columns is-centered has-text-centered" style="margin-top: 10px;">
            <div class="column is-three-quarters">
              <div class="subtitle is-6 has-text-centered" style="margin-bottom: 2px;">
                <span style="font-weight: 600;">Speed:</span> 
                <span id="speedValue1" style="font-weight: 400;">5.0</span>x
              </div>
              <input type="range" min="0.1" max="10.0" step="0.1" value="5.0" 
                     style="width: 80%; cursor: pointer; accent-color: #363636;"
                     oninput="updateVideoSpeed('video1', 'speedValue1', this.value)">
            </div>
          </div>
        </div>

        <div class="item item-video2">
          <video poster="" id="video2" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/second_web.mp4" type="video/mp4">
          </video>
          <div class="columns is-centered has-text-centered" style="margin-top: 10px;">
            <div class="column is-three-quarters">
              <div class="subtitle is-6 has-text-centered" style="margin-bottom: 2px;">
                <span style="font-weight: 600;">Speed:</span> 
                <span id="speedValue2" style="font-weight: 400;">5.0</span>x
              </div>
              <input type="range" min="0.1" max="10.0" step="0.1" value="5.0" 
                     style="width: 80%; cursor: pointer; accent-color: #363636;"
                     oninput="updateVideoSpeed('video2', 'speedValue2', this.value)">
            </div>
          </div>
        </div>

        <div class="item item-video3">
          <video poster="" id="video3" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/third_web.mp4" type="video/mp4">
          </video>
          <div class="columns is-centered has-text-centered" style="margin-top: 10px;">
            <div class="column is-three-quarters">
              <div class="subtitle is-6 has-text-centered" style="margin-bottom: 2px;">
                <span style="font-weight: 600;">Speed:</span> 
                <span id="speedValue2" style="font-weight: 400;">5.0</span>x
              </div>
              <input type="range" min="0.1" max="10.0" step="0.1" value="5.0" 
                     style="width: 80%; cursor: pointer; accent-color: #363636;"
                     oninput="updateVideoSpeed('video3', 'speedValue3', this.value)">
            </div>
          </div>
        </div>

        <div class="item item-video4">
          <video poster="" id="video4" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/fourth_web.mp4" type="video/mp4">
          </video>
          <div class="columns is-centered has-text-centered" style="margin-top: 10px;">
            <div class="column is-three-quarters">
              <div class="subtitle is-6 has-text-centered" style="margin-bottom: 2px;">
                <span style="font-weight: 600;">Speed:</span> 
                <span id="speedValue2" style="font-weight: 400;">5.0</span>x
              </div>
              <input type="range" min="0.1" max="10.0" step="0.1" value="5.0" 
                     style="width: 80%; cursor: pointer; accent-color: #363636;"
                     oninput="updateVideoSpeed('video4', 'speedValue4', this.value)">
            </div>
          </div>
        </div>
        
        <div class="item item-video5">
          <video poster="" id="video5" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/fifth_web.mp4" type="video/mp4">
          </video>
          <div class="columns is-centered has-text-centered" style="margin-top: 10px;">
            <div class="column is-three-quarters">
              <div class="subtitle is-6 has-text-centered" style="margin-bottom: 2px;">
                <span style="font-weight: 600;">Speed:</span> 
                <span id="speedValue2" style="font-weight: 400;">5.0</span>x
              </div>
              <input type="range" min="0.1" max="10.0" step="0.1" value="5.0" 
                     style="width: 80%; cursor: pointer; accent-color: #363636;"
                     oninput="updateVideoSpeed('video5', 'speedValue5', this.value)">
            </div>
          </div>
        </div>

      </div>
    </div>
  </div>
</section>

<script>
  function updateVideoSpeed(videoId, labelId, speed) {
    var video = document.getElementById(videoId);
    var label = document.getElementById(labelId);
    video.playbackRate = speed;
    label.innerText = parseFloat(speed).toFixed(1);
  }

  document.addEventListener('DOMContentLoaded', function() {
    var videoIds = ['video1', 'video2', 'video3', 'video4', 'video5'];
    
    videoIds.forEach(function(id) {
      var video = document.getElementById(id);
      if (video) {
        video.playbackRate = 5.0;
      }
    });
  });
</script>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop is-centered has-text-centered">
      <h2 class="title is-2">Comparison with Baseline</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="compare-video1" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/compare_first.mp4" type="video/mp4">
          </video>
          
        </div>

        <div class="item item-video2">
          <video poster="" id="compare-video2" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/compare_second.mp4" type="video/mp4">
          </video>
        </div>

        <div class="item item-video3">
          <video poster="" id="compare-video3" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/compare_third.mp4" type="video/mp4">
          </video>
        </div>

        <div class="item item-video4">
          <video poster="" id="compare-video4" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/compare_fourth.mp4" type="video/mp4">
          </video>
        </div>

        <div class="item item-video5">
          <video poster="" id="compare-video5" controls muted loop height="100%" preload="metadata">
            <source src="static/videos/compare_fifth.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End video carousel -->



<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-2 is-centered has-text-centered">Method Overview</h2>
       <div class="item mb-3">
        <img src="static/images/method.png" alt="Method overview" loading="lazy"/>
        <div class="has-text-justified">
          <p>
            Our framework leverages a <b>conditional diffusion model</b> to generate human motion, conditioned on <b>head trajectory</b> and <b>intermittently visible hand poses</b> from egocentric video. To efficiently process long sequences, we incorporate a <b>local attention</b> mechanism within the encoder-decoder transformer architecture. Furthermore, we encode <b>sequence-level context</b> such as <b>body shape</b> and <b>field-of-view</b> to ensure consistent reconstruction across diverse device configurations.
          </p>
        </div>
      </div>


       <div class="item">
        <img src="static/images/augumentation.png" alt="Augmentation" loading="lazy"/>
        <div class="has-text-justified">
          <p>
            To overcome the lack of datasets, we introduce a novel augmentation method that simulates real-world egocentric conditions. We apply <b>spatial augmentation</b> to <b>mimic diverse device field-of-views</b>, and <b>temporal augmentation</b> via <b>independent event duration sampling</b> to realistically model mis-detection patterns.</p>
        </div>
      </div>
  </div>
</div>
</div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <div class="bibtex-header">
        <h2 class="title">BibTeX</h2>
        <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
          <i class="fas fa-copy"></i>
          <span class="copy-text">Copy</span>
        </button>
      </div>
      <pre id="bibtex-code"><code>@article{cho2025hamos,
  title={Hand-Aware Egocentric Motion Reconstruction with Sequence-Level Context},
  author={Cho, Kyungwon and Joo, Hanbyul},
  journal={arXiv preprint arXiv:2512.19283},
  year={2025},
}</code></pre>
        
    </div>
</section>
<!--End BibTex citation -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
